
# 集群模式概述

为了更加简单地理解Spark涉及的组成部分，这个文档给出了一个对于Spark如何在集群上运行的简短概述。阅读[应用提交指南](http://spark.apache.org/docs/2.0.1/submitting-applications.html)来学习在一个集群上运行应用。

组成部分
---

Spark应用是作为独立的处理集合运行在一个集群上，通过主函序（也叫驱动程序，Driver Program）里的SparkContext对象进行协调。


具体来说，Spark要运行在集群上，SparkContext能够连接到集群管理（Spark的standalone cluster管理Mesos或YARN），它负责对应用分配资源。一旦连接之后，Spark获取集群上节点的执行器（executors），执行器负责计算和存储我们应用的数据。接下来，Spark发送你的应用代码（jar文件）到执行器。最后，SparkContext发送任务（task）到执行器运行。


![Spark Components](http://spark.apache.org/docs/2.0.1/img/cluster-overview.png)

以下是一些对这个架构的解释有帮助的东西：

1. 每个应用在整个应用的生命期，获取它自己拥有的执行器过程，并且以多线程方式运行任务。这有利于在调度方面（每个driver调度它自己的task）和执行器方面（来自不同application的tasks运行在不同的JVMs里）隔离来自不同的应用。然而，这也意味着，在不同的Spark application（SparkContext实例）之间数据不能共享，除非写数据到一个外部存储系统。

Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.

Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).

The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes.

Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.
